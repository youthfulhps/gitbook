# 1. Search Engine

_검색 엔진의 예시로 구글 검색 엔진이 어떻게 동작하는 지 알아보자._ 구글은 크롤링, 색인 생성, 검색결과 게재 세 단계로 진행된다.

#### 크롤링

크롤링은 웹에 어떤 페이지가 존재하는 지 파악하는 일련의 과정이다. 검색 엔진은 특정 웹의 새 페이지가 생성되었는 지, 업데이트되었는 지 파악하기 위해 지속적으로 페이지를 검색하고 파악된 페이지 목록에 추가한다. 이 과정을 _URL 검색_ 이라고 하는데, 여기서 새 페이지가 생성되었는 지 파악할 때는, 기존에 파악된 페이지에서 새로운 페이지로 연결되는 링크를 통해 새로운 페이지가 추가되었음을 파악하고 발견되거나, 사이트 소유자가 사이트맵을 제출하여 페이지를 발견한다.

새로운 페이지가 발견되면 파악된 페이지로 추가되고, 이를 방문 (= 크롤링)하기 시작한다. 크롤러는 수십억 개의 페이지를 크롤링하기 때문에 과부하를 막기위해 크롤링 빈도를 정하고, 각 사이트에서 크롤링 타겟으로 하는 페이지 수를 결정한다.

\_sitemap.xml에서 특정 URL의 페이지가 변경되는 빈도를 나타내는 태그와 크롤링 빈도가 관련있을 까 했는데, 'hourly' 보다 'yearly'로 표기된 페이지를 더 자주 크롤링할 수 있다고 한다. 극단적으로 절대 변하지 않음을 나타내는 'never' 라고 표기되어 있지만, 예기치 않게 발생한 변경 사항을 처리하기 위해 크롤링될 수 있다.

TMI로, 로 지정한 URL의 우선순위는 검색 엔진에게는 같은 사이트에 있는 URL 중에서의 우선순위를 나타내는 것이지, 검색 엔진에게 더 높은 우선순위를 부여받는 것은 아니라고 한다. \_

또한, 모든 페이지를 크롤링하지 않고, 중복 가능성이 있는 페이지, 소유자가 크롤링을 허용하지 않은 페이지는 크롤링 대상에서 제외된다.

_소유자 입장에서 크롤링을 허용하고 싶지 않은 페이지가 있다면, 가령 어드민이나 사업자 내부 에서 사용되는 페이지 정도? 라면 robots.txt 파일의 `disallow` 지시어를 사용하여 크롤러가 엑세스하면 안되는 경로를 지정할 수 있다._

```
// robots.txt

User-agent: *
Disallow: /includes/

...

Sitemap: 
```

크롤링하는 동안 크롬을 사용하여 페이지를 랜더링해보고, 발견된 자바스크립트를 실행한다.

#### 크롤러의 자바스크립트 처리 방식

[![how-googlebot-processes-javascript](https://camo.githubusercontent.com/6c1dbd0da8eae32a994085c33396905e36166809f0fd023e32dbfa1e4801a827/68747470733a2f2f646576656c6f706572732e676f6f676c652e636f6d2f7374617469632f7365617263682f646f63732f616476616e6365642f696d616765732f676f6f676c65626f742d637261776c2d72656e6465722d696e6465782e706e673f686c3d6b6f)](https://camo.githubusercontent.com/6c1dbd0da8eae32a994085c33396905e36166809f0fd023e32dbfa1e4801a827/68747470733a2f2f646576656c6f706572732e676f6f676c652e636f6d2f7374617469632f7365617263682f646f63732f616476616e6365642f696d616765732f676f6f676c65626f742d637261776c2d72656e6465722d696e6465782e706e673f686c3d6b6f)

페이지를 크롤링 및 랜더링 대기열에 추가하고 순서가 되면 HTTP 요청을 통해 크롤링 대기열에서 URL을 가져오는데, 이 때 구글봇이 robots.txt를 읽고, 본 페이지가 크롤링을 허용하고 있는 지 에 대한 여부를 판단한다. 크롤링된 페이지에서 `href` 속성을 통해 다른 URL에 대한 응답을 확인하고 크롤링 대기열에 링크되어 있는 다른 URL의 타겟 페이지를 크롤링 대기열에 추가한다.

_좀 많이 길어서 따로 정리가 필요하다._

#### 색인 생성

페이지가 크롤링되면 페이지의 내용을 파악하는 단계에 들어간다. 이 단계를 색인 생성이라 하고,

`<title>` 요소에 포함되어 있는 컨텐츠를 시작으로 alt, 이미지, 동영상 등 텍스트와 핵심 컨텐츠 태그들을 중심으로 속성을 처리하고 페이지를 분석한다.

_색인 생성 단계에서 다른 페이지와 중복된 페이지인지를 판단한다. 개인 블로그에서 URL을 변경한 적이 있었는데, 변경 전 페이지와 변경 후 페이지가 중복으로 검색되지 않는 것을 보면 색인 생성 단계에서 반려된듯 하다._

_이것도 잠시 궁금하니까 어떤 URL을 중복되었다고 판단하는 지 살펴보고 가자._

\_페이지가 서로 완전히 일치해야 중복으로 간주되는 것이 아니라, 색인 생성 단계에서 페이지의 핵심 컨텐츠 태그들을 중심으로 속성을 처리하고 페이지를 분석할 때 주요 컨텐츠들이 유사하다고 판단되면, 중복된 페이지라고 간주한다고 한다. 또한 여러 언어 버전의 페이지가 존재한다면, 주요 컨텐츠의 언어가 같을 때 이를 중복으로 간주한다.

하지만, 가장 일반적으로 중복된 페이지로 간주되는 경우는 여러 기기를 지원하기 위한 여러 페이지가 존재하거나, 세션 아이디 혹은 매개변수를 포함하고 있는 페이지가 그 예시라고 한다.\_

```
https://example.com/news
https://m.example.com/news
https://amp.example.com/news
```

```
https://www.example.com/products?category=dresses&color=green
https://example.com/dresses/cocktail?gclid=ABCD
https://www.example.com/dresses/green/greendress.html
```

다시 돌아와서, 중복된 페이지들 중 혹은 온전한 표준 페이지에서 가장 컨텐츠를 잘 대표하는 페이지를 선택한다.

_잘? 이면, m., amp.과 같이 특정 기기를 위한 페이지가 아니라, 일반적인 example 페이지를 선택한다?_

선택된 표준 페이지와 그 컨텐츠에 관련 정보들을 수집하고 이를 검색 결과에 페이지를 게재하는 단계에서 사용된다. 일부 정보들은 페이지의 언어, 컨텐츠가 속하는 국가, 사용성 등이 포함된다. 하지만, 여기서 또 무조건 색인을 생성하는 것은 아닌데, 가령 페이지 컨텐의 품질이 낮거나, robots.txt에 색인 생성 거절에 대한 지시어가 포함되어 있거나, 웹 사이트 디자인으로 인해 색인 생성이 어려운 경우 색인 생성이 반려된다.

#### 검색결과 게재

이제, 사용자가 검색어를 입력하면 생성된 색인에서 일치하는 페이지들을 모아 품질이 높고, 사용자 검색에 가장 관련성이 높다고 판단되는 결과를 반환한다. 사용자의 위치나 언어, 기기와 같은 정보를 포함하여 수많은 요인들로 인해 그 순위가 결정된다.

여기서도 마찬가지로, 페이지 컨텐츠가 사용자와 관련없거나, 컨텐츠 품질이 낮거나, 로봇 메타 지시어를 통해 게재가 차단되면 검색 결과 게재가 반려된다.

#### Reference

* [https://developers.google.com/search?hl=ko](https://developers.google.com/search?hl=ko)
* [https://www.sitemaps.org/ko/protocol.html](https://www.sitemaps.org/ko/protocol.html)
